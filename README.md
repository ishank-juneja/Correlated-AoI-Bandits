# Readme

The paper refereed to - Multi-Armed Bandits with Correlated Arms - considers general correlated bandit instances with rewards bounded in the range [0, B], B \in Reals. However in this work on correlated bandit instances, only bandit instances with 0-1 rewards are considered. This means that the usual, or more standard version of MAB algorithms can be used.